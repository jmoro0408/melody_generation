# Melody Generation
## Creating new melodies with RNNs.


This program uses the [ESAC Dataset](http://www.esac-data.org/) to generate new melodies based on simple folk songs using a recurrent neural network. It is a code along from The Sound of AI ["Melody Generation with RNN-LSTM"](https://www.youtube.com/playlist?list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz) youtube series. 

The program relies heavily on the Music21 library to manipulate .krn data into a format that is processable by the neural net. The program first imports the .krn data, checks if all notes are of an acceptable duration (from a whole note to a sixteenth note), transposes the song to either Cmaj or Amin (to simplify the model), and encodes the song to a time-series representation. 
The time series representation uses the midi-style integer representation for the note pitch (i.e C4 -> 60) and underscores (_ _ _) for duration in sixteenth notes, rests are represented by "r" with underscore for their durations. 

For example an E4 crotchet is represented as [64 _ _ _ ]. 
Each song in the dataset is combined to a single string to be fed into the RNN. 

The algorithm is trained using a single layer Tensorflow LSTM RN. Each symbol in the dataset string is first mapped to an integer (i.e "_" -> 9, "r" -> 30) as the RNN can only process numbers. The model is trained for 50 epochs with spare categorical crossentropy loss and the adam optimizer, hyperparameters are available in the /train.py file. 

Note: This LSTM was prediction an approximate 20 mins/epoch on my old 2013 macbook pro, approximately 16 hours for all 50 epochs. To speed this up,  I copied the train.py file to a Google Colab notebook and uploaded the output from the Preprocess file to Google Drive. This way I could take advantage of the Google Colab GPU Runtime, reducing the training from 20 mins/epoch top <1 min/epoch.

The model was then used to make predictions of future melodies when provided a sample of seed notes. Temperature (degree of "randomness") and the duration of the predicted melody can be controlled by the "temperature" and "Sequence_Length" constants. 


The prediction ifs then converted back to a midi file and visualized using MuseScore 3. Take a look at some of the predicted outputs below!

## Examples of Outputs
The first six notes (blue) are seeds from the beginning of dataset songs, and the following notes are generated by the RNN. 

https://user-images.githubusercontent.com/66977019/120243681-21181a80-c21d-11eb-9718-557f53f6af29.mov

https://user-images.githubusercontent.com/66977019/120243552-bf57b080-c21c-11eb-9d1a-28fcca5efa99.mov


https://user-images.githubusercontent.com/66977019/120243688-237a7480-c21d-11eb-9cce-81840fbbaa96.mov


https://user-images.githubusercontent.com/66977019/120243691-25dcce80-c21d-11eb-9692-d1e4b0816b01.mov


https://user-images.githubusercontent.com/66977019/120243696-283f2880-c21d-11eb-9318-d3d8dceac0a9.mov

